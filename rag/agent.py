"""
Agent implementation responsible for orchestrating a retrieval-augmented
generation (RAG) workflow.

The agent:
- Orchestrates retrieval and generation steps
- Formats prompts using retrieved context
- Assembles source citations
- Produces a structured execution log for observability and debugging
"""

import uuid
import time

class Agent:
    """
    Orchestrates a simple RAG pipeline.

    This agent coordinates:
    - Document retrieval
    - Prompt construction
    - LLM invocation
    - Citation attachment
    - Execution logging (latency, trace ID, retrieved sources)
    """
    def __init__(self, retriever, llm):
        """
        Initialize the agent with its dependencies.

        :param retriever: Component responsible for retrieving relevant documents.
                          Expected to expose a `retrieve(question, top_k)` method.
        :param llm: Callable language model used to generate the answer from a prompt.
        """
        self.retriever = retriever
        self.llm = llm

    def run(self, question: str, top_k: int = 3):
        """
        Execute the full RAG workflow for a given question.

        The workflow consists of:
        1. Retrieving relevant chunks from the documents
        2. Drafting an answer using an LLM
        3. Attaching citations to the generated answer

        A structured log object is returned alongside the final response
        to support observability, debugging, and evaluation.

        :param question: User question to be answered.
        :param top_k: Number of top chunks to retrieve.
        :return: A tuple of (final_response, execution_log).
        """
        trace_id = str(uuid.uuid4())

        plan = ["retrieve", "draft", "cite"]
        errors = []

        # Retrieve
        start_time = time.time()
        retrieved = self.retriever.retrieve(question, top_k)
        retrieve_latency = int((time.time() - start_time) * 1000)

        # Draft
        draft_start_time = time.time()
        prompt, sources = self._create_prompt(question, retrieved)
        answer = self.llm(prompt)
        draft_latency = int((time.time() - draft_start_time) * 1000)

        # Cite
        response = self._add_citations(answer, sources)

        total_latency = int((time.time() - start_time) * 1000)

        log = {
            "trace_id": trace_id,
            "question": question,
            "plan": plan,
            "retrieval": [
                {
                    "file": r[0]["metadata"]["file"],
                    "chunk_id": r[0]["metadata"]["chunk_id"],
                    "text": r[0]["text"],
                    "score": round(float(r[1]), 4),
                    "section_path": r[0]["metadata"].get("section_path", [])
                }
                for r in retrieved
            ],
            "draft_tokens": len(prompt.split()),
            "latency_ms": {
                "retrieve": retrieve_latency,
                "draft": draft_latency,
                "total": total_latency
            },
            "errors": errors
        }

        return response, log

    def _create_prompt(self, question, retrieved):
        """
        Construct an LLM prompt using retrieved chunks as context.

        Each retrieved chunk is assigned a source identifier that can
        later be referenced in the generated citations.

        :param question: User question.
        :param retrieved: Retrieved chunks with relevance scores.
        :return: A tuple of (prompt, sources), where sources contain
                 citation metadata.
        """
        context_blocks = []
        sources = []

        for i, (document, score) in enumerate(retrieved):
            metadata = document["metadata"]
            source_id = f"[{i + 1}]"

            context_blocks.append(
                f"{source_id} File: {metadata['file']}, Page: {metadata['page']}\n{document['text']}"
            )

            sources.append({
                "id": source_id,
                "file": metadata["file"],
                "page": metadata["page"]
            })

        context = "\n\n".join(context_blocks)

        prompt = f"""
You are a QA assistant.
Answer the question **only** using the provided context.
If the answer is not contained, say "I don't know".

Context:
{context}

Question:
{question}
"""
        return prompt.strip(), sources

    def _add_citations(self, answer: str, sources):
        """
        Append source citations to the generated answer.

        :param answer: Answer generated by the LLM.
        :param sources: List of source metadata collected during retrieval.
        :return: Final answer with formatted citations.
        """
        citation_lines = "\n\nSources:\n"
        for s in sources:
            citation_lines += f"{s['id']} {s['file']} (page {s['page']})\n"

        return answer.strip() + citation_lines
